{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73332f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ta\n",
      "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: shap in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (0.44.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from ta) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from ta) (2.2.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from shap) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from shap) (1.2.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: numba in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from shap) (0.61.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from numba->shap) (0.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from pandas->ta) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from pandas->ta) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from pandas->ta) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from scikit-learn->shap) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\timothyka\\avenir-hku-web\\venv\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Building wheels for collected packages: ta\n",
      "  Building wheel for ta (pyproject.toml): started\n",
      "  Building wheel for ta (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29497 sha256=822ce5285253e093c443ac1b0009516ec0dc067ddb126ee59cb48cecd55e214a\n",
      "  Stored in directory: c:\\users\\timothyka\\appdata\\local\\pip\\cache\\wheels\\5c\\a1\\5f\\c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
      "Successfully built ta\n",
      "Installing collected packages: ta\n",
      "Successfully installed ta-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ta torch shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb04708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data directory contents: ['1000000MOGUSDT.parquet', '1000BONKUSDT.parquet', '1000CATUSDT.parquet', '1000CHEEMSUSDT.parquet', '1000FLOKIUSDT.parquet', '1000LUNCUSDT.parquet', '1000PEPEUSDT.parquet', '1000RATSUSDT.parquet', '1000SATSUSDT.parquet', '1000SHIBUSDT.parquet', '1000WHYUSDT.parquet', '1000XECUSDT.parquet', '1000XUSDT.parquet', '1INCHUSDT.parquet', '1MBABYDOGEUSDT.parquet', 'AAVEUSDT.parquet', 'ACEUSDT.parquet', 'ACHUSDT.parquet', 'ACTUSDT.parquet', 'ACXUSDT.parquet', 'ADAUSDT.parquet', 'AEROUSDT.parquet', 'AEVOUSDT.parquet', 'AGIXUSDT.parquet', 'AGLDUSDT.parquet', 'AI16ZUSDT.parquet', 'AIUSDT.parquet', 'AIXBTUSDT.parquet', 'AKTUSDT.parquet', 'ALCHUSDT.parquet', 'ALGOUSDT.parquet', 'ALICEUSDT.parquet', 'ALPHAUSDT.parquet', 'ALTUSDT.parquet', 'AMBUSDT.parquet', 'ANIMEUSDT.parquet', 'ANKRUSDT.parquet', 'APEUSDT.parquet', 'API3USDT.parquet', 'APTUSDT.parquet', 'ARBUSDT.parquet', 'ARCUSDT.parquet', 'ARKMUSDT.parquet', 'ARPAUSDT.parquet', 'ARUSDT.parquet', 'ASTRUSDT.parquet', 'ATAUSDT.parquet', 'ATOMUSDT.parquet', 'AUCTIONUSDT.parquet', 'AVAAIUSDT.parquet', 'AVAUSDT.parquet', 'AVAXUSDT.parquet', 'AXLUSDT.parquet', 'AXSUSDT.parquet', 'BAKEUSDT.parquet', 'BANANAUSDT.parquet', 'BANDUSDT.parquet', 'BANUSDT.parquet', 'BATUSDT.parquet', 'BBUSDT.parquet', 'BCHUSDT.parquet', 'BEAMXUSDT.parquet', 'BELUSDT.parquet', 'BICOUSDT.parquet', 'BIGTIMEUSDT.parquet', 'BIOUSDT.parquet', 'BLURUSDT.parquet', 'BLZUSDT.parquet', 'BNBUSDT.parquet', 'BNXUSDT.parquet', 'BOMEUSDT.parquet', 'BONDUSDT.parquet', 'BRETTUSDT.parquet', 'BSVUSDT.parquet', 'BTCDOMUSDT.parquet', 'BTCUSDT.parquet', 'C98USDT.parquet', 'CAKEUSDT.parquet', 'CATIUSDT.parquet', 'CELOUSDT.parquet', 'CELRUSDT.parquet', 'CETUSUSDT.parquet', 'CFXUSDT.parquet', 'CGPTUSDT.parquet', 'CHILLGUYUSDT.parquet', 'CHRUSDT.parquet', 'CHZUSDT.parquet', 'CKBUSDT.parquet', 'COMPUSDT.parquet', 'COOKIEUSDT.parquet', 'COTIUSDT.parquet', 'COWUSDT.parquet', 'CRVUSDT.parquet', 'CTKUSDT.parquet', 'CTSIUSDT.parquet', 'CVCUSDT.parquet', 'CVXUSDT.parquet', 'CYBERUSDT.parquet', 'DARUSDT.parquet', 'DASHUSDT.parquet', 'DEFIUSDT.parquet', 'DEGENUSDT.parquet', 'DEGOUSDT.parquet', 'DENTUSDT.parquet', 'DEXEUSDT.parquet', 'DFUSDT.parquet', 'DGBUSDT.parquet', 'DIAUSDT.parquet', 'DODOXUSDT.parquet', 'DOGEUSDT.parquet', 'DOGSUSDT.parquet', 'DOTUSDT.parquet', 'DRIFTUSDT.parquet', 'DUSDT.parquet', 'DUSKUSDT.parquet', 'DYDXUSDT.parquet', 'DYMUSDT.parquet', 'EDUUSDT.parquet', 'EGLDUSDT.parquet', 'EIGENUSDT.parquet', 'ENAUSDT.parquet', 'ENJUSDT.parquet', 'ENSUSDT.parquet', 'EOSUSDT.parquet', 'ETCUSDT.parquet', 'ETHFIUSDT.parquet', 'ETHUSDT.parquet', 'ETHWUSDT.parquet', 'FARTCOINUSDT.parquet', 'FETUSDT.parquet', 'FIDAUSDT.parquet', 'FILUSDT.parquet', 'FLOWUSDT.parquet', 'FLUXUSDT.parquet', 'FTMUSDT.parquet', 'FXSUSDT.parquet', 'GALAUSDT.parquet', 'GASUSDT.parquet', 'GLMRUSDT.parquet', 'GLMUSDT.parquet', 'GMTUSDT.parquet', 'GMXUSDT.parquet', 'GOATUSDT.parquet', 'GRASSUSDT.parquet', 'GRIFFAINUSDT.parquet', 'GRTUSDT.parquet', 'GTCUSDT.parquet', 'GUSDT.parquet', 'HBARUSDT.parquet', 'HFTUSDT.parquet', 'HIGHUSDT.parquet', 'HIPPOUSDT.parquet', 'HIVEUSDT.parquet', 'HMSTRUSDT.parquet', 'HOOKUSDT.parquet', 'HOTUSDT.parquet', 'ICPUSDT.parquet', 'IDUSDT.parquet', 'ILVUSDT.parquet', 'IMXUSDT.parquet', 'INJUSDT.parquet', 'IOSTUSDT.parquet', 'IOTAUSDT.parquet', 'IOTXUSDT.parquet', 'IOUSDT.parquet', 'JOEUSDT.parquet', 'JTOUSDT.parquet', 'JUPUSDT.parquet', 'KAIAUSDT.parquet', 'KASUSDT.parquet', 'KAVAUSDT.parquet', 'KDAUSDT.parquet', 'KEYUSDT.parquet', 'KLAYUSDT.parquet', 'KMNOUSDT.parquet', 'KNCUSDT.parquet', 'KOMAUSDT.parquet', 'KSMUSDT.parquet', 'LDOUSDT.parquet', 'LINKUSDT.parquet', 'LISTAUSDT.parquet', 'LITUSDT.parquet', 'LOOMUSDT.parquet', 'LPTUSDT.parquet', 'LQTYUSDT.parquet', 'LRCUSDT.parquet', 'LTCUSDT.parquet', 'LUMIAUSDT.parquet', 'LUNA2USDT.parquet', 'MAGICUSDT.parquet', 'MANAUSDT.parquet', 'MANTAUSDT.parquet', 'MASKUSDT.parquet', 'MAVIAUSDT.parquet', 'MAVUSDT.parquet', 'MBOXUSDT.parquet', 'MELANIAUSDT.parquet', 'MEMEUSDT.parquet', 'METISUSDT.parquet', 'MEUSDT.parquet', 'MEWUSDT.parquet', 'MINAUSDT.parquet', 'MKRUSDT.parquet', 'MOCAUSDT.parquet', 'MOODENGUSDT.parquet', 'MORPHOUSDT.parquet', 'MOVEUSDT.parquet', 'MOVRUSDT.parquet', 'MYROUSDT.parquet', 'NEARUSDT.parquet', 'NEIROETHUSDT.parquet', 'NEIROUSDT.parquet', 'NEOUSDT.parquet', 'NFPUSDT.parquet', 'NMRUSDT.parquet', 'NOTUSDT.parquet', 'NTRNUSDT.parquet', 'OCEANUSDT.parquet', 'OGNUSDT.parquet', 'OMGUSDT.parquet', 'OMNIUSDT.parquet', 'OMUSDT.parquet', 'ONDOUSDT.parquet', 'ONEUSDT.parquet', 'ONTUSDT.parquet', 'OPUSDT.parquet', 'ORBSUSDT.parquet', 'ORCAUSDT.parquet', 'ORDIUSDT.parquet', 'OXTUSDT.parquet', 'PENDLEUSDT.parquet', 'PENGUUSDT.parquet', 'PEOPLEUSDT.parquet', 'PHAUSDT.parquet', 'PHBUSDT.parquet', 'PIPPINUSDT.parquet', 'PIXELUSDT.parquet', 'PNUTUSDT.parquet', 'POLUSDT.parquet', 'POLYXUSDT.parquet', 'PONKEUSDT.parquet', 'POPCATUSDT.parquet', 'PORTALUSDT.parquet', 'POWRUSDT.parquet', 'PROMUSDT.parquet', 'PYTHUSDT.parquet', 'QNTUSDT.parquet', 'QTUMUSDT.parquet', 'QUICKUSDT.parquet', 'RADUSDT.parquet', 'RAREUSDT.parquet', 'RAYSOLUSDT.parquet', 'RAYUSDT.parquet', 'RDNTUSDT.parquet', 'REEFUSDT.parquet', 'REIUSDT.parquet', 'RENDERUSDT.parquet', 'RENUSDT.parquet', 'REZUSDT.parquet', 'RIFUSDT.parquet', 'RLCUSDT.parquet', 'RONINUSDT.parquet', 'ROSEUSDT.parquet', 'RPLUSDT.parquet', 'RSRUSDT.parquet', 'RUNEUSDT.parquet', 'RVNUSDT.parquet', 'SAFEUSDT.parquet', 'SAGAUSDT.parquet', 'SANDUSDT.parquet', 'SANTOSUSDT.parquet', 'SCRTUSDT.parquet', 'SCRUSDT.parquet', 'SCUSDT.parquet', 'SEIUSDT.parquet', 'SKLUSDT.parquet', 'SLERFUSDT.parquet', 'SLPUSDT.parquet', 'SNXUSDT.parquet', 'SOLUSDT.parquet', 'SOLVUSDT.parquet', 'SONICUSDT.parquet', 'SPELLUSDT.parquet', 'SPXUSDT.parquet', 'SSVUSDT.parquet', 'STEEMUSDT.parquet', 'STGUSDT.parquet', 'STMXUSDT.parquet', 'STORJUSDT.parquet', 'STRAXUSDT.parquet', 'STRKUSDT.parquet', 'STXUSDT.parquet', 'SUIUSDT.parquet', 'SUNUSDT.parquet', 'SUPERUSDT.parquet', 'SUSDT.parquet', 'SUSHIUSDT.parquet', 'SWARMSUSDT.parquet', 'SWELLUSDT.parquet', 'SXPUSDT.parquet', 'SYNUSDT.parquet', 'SYSUSDT.parquet', 'TAOUSDT.parquet', 'THETAUSDT.parquet', 'THEUSDT.parquet', 'TIAUSDT.parquet', 'TLMUSDT.parquet', 'TNSRUSDT.parquet', 'TOKENUSDT.parquet', 'TONUSDT.parquet', 'TRBUSDT.parquet', 'TRUMPUSDT.parquet', 'TRUUSDT.parquet', 'TRXUSDT.parquet', 'TURBOUSDT.parquet', 'TUSDT.parquet', 'TWTUSDT.parquet', 'UMAUSDT.parquet', 'UNFIUSDT.parquet', 'UNIUSDT.parquet', 'USDCUSDT.parquet', 'USTCUSDT.parquet', 'USUALUSDT.parquet', 'UXLINKUSDT.parquet', 'VANAUSDT.parquet', 'VANRYUSDT.parquet', 'VELODROMEUSDT.parquet', 'VETUSDT.parquet', 'VINEUSDT.parquet', 'VIRTUALUSDT.parquet', 'VTHOUSDT.parquet', 'VVVUSDT.parquet', 'WAVESUSDT.parquet', 'WAXPUSDT.parquet', 'WIFUSDT.parquet', 'WLDUSDT.parquet', 'WOOUSDT.parquet', 'WUSDT.parquet', 'XAIUSDT.parquet', 'XEMUSDT.parquet', 'XLMUSDT.parquet', 'XMRUSDT.parquet', 'XRPUSDT.parquet', 'XTZUSDT.parquet', 'XVGUSDT.parquet', 'XVSUSDT.parquet', 'YFIUSDT.parquet', 'YGGUSDT.parquet', 'ZENUSDT.parquet', 'ZEREBROUSDT.parquet', 'ZETAUSDT.parquet', 'ZILUSDT.parquet', 'ZKUSDT.parquet', 'ZROUSDT.parquet', 'ZRXUSDT.parquet']\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, os, time\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import shap\n",
    "\n",
    "PROCESSES = mp.Pool(4)\n",
    "BASE_DIR = os.getcwd()\n",
    "TRAIN_DATA_DIR = os.path.join(BASE_DIR, \"kline_data\", \"train_data\")\n",
    "SUBMISSION_ID_PATH = os.path.join(BASE_DIR, \"submission_id.csv\")\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"data_cache\")\n",
    "\n",
    "class OptimizedModel:\n",
    "    def __init__(self):\n",
    "        self.train_data_path = TRAIN_DATA_DIR\n",
    "        self.submission_id_path = SUBMISSION_ID_PATH\n",
    "        self.start_datetime = datetime.datetime(2021, 3, 1, 0, 0, 0)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "    \n",
    "    def get_all_symbol_list(self):\n",
    "        try:\n",
    "            parquet_name_list = os.listdir(self.train_data_path)\n",
    "            symbol_list = [parquet_name.split(\".\")[0] for parquet_name in parquet_name_list]\n",
    "            return symbol_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_all_symbol_list: {e}\")\n",
    "            return []\n",
    "\n",
    "    def compute_factors_torch(self, df):\n",
    "        # 转换为张量 convert to tensor\n",
    "        close = torch.tensor(df['close_price'].values, dtype=torch.float32, device=self.device)\n",
    "        volume = torch.tensor(df['volume'].values, dtype=torch.float32, device=self.device)\n",
    "        amount = torch.tensor(df['amount'].values, dtype=torch.float32, device=self.device)\n",
    "        high = torch.tensor(df['high_price'].values, dtype=torch.float32, device=self.device)\n",
    "        low = torch.tensor(df['low_price'].values, dtype=torch.float32, device=self.device)\n",
    "        buy_volume = torch.tensor(df['buy_volume'].values, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # VWAP - Volume-Weighted Average Price: https://www.investopedia.com/terms/v/vwap.asp\n",
    "        ## Average price of the security has traded at throughout the day weighted by volume\n",
    "        vwap = torch.where(volume > 0, amount / volume, close)\n",
    "        vwap = torch.where(torch.isfinite(vwap), vwap, close)\n",
    "        \n",
    "        # RSI - Relative Strength Index (Full Implementation, 14 day window): https://www.investopedia.com/terms/r/rsi.asp\n",
    "        ## Momentum indicator to measure how fast & how much a stock moved recently\n",
    "        ## RSI > 70 -> Asset is overbought, possible trend reversal\n",
    "        ## RSI < 30 -> Asset maybe oversold, possible reversal outward\n",
    "        delta = torch.diff(close, prepend=close[:1]) # calculate delta between consecutive elements\n",
    "        gain = torch.where(delta > 0, delta, torch.tensor(0.0, device=self.device))\n",
    "        loss = torch.where(delta < 0, -delta, torch.tensor(0.0, device=self.device))\n",
    "        \n",
    "        init_gain = gain[:14].mean() if len(gain) > 14 else torch.tensor(0.0, device=self.device)\n",
    "        init_loss = loss[:14].mean() if len(gain) > 14 else torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        avg_gain = torch.zeros_like(close, device=self.device)\n",
    "        avg_loss = torch.zeros_like(close, device=self.device)\n",
    "        avg_gain[:14] = init_gain\n",
    "        avg_loss[:14] = init_loss\n",
    "        for i in range(14, len(close)): # Wilder's smoothing method -> less reactive than standard ema, more stable\n",
    "            avg_gain[i] = (avg_gain[i-1] * 13 + gain[i]) / 14\n",
    "            avg_loss[i] = (avg_loss[i-1] * 13 + loss[i]) / 14\n",
    "        \n",
    "        rs = torch.where(avg_loss > 0, avg_gain / avg_loss, torch.tensor(0.0, device=self.device))\n",
    "        rsi = 100 - 100 / (1 + rs)\n",
    "        rsi = torch.where(torch.isnan(rsi), torch.tensor(50.0, device=self.device), rsi) # set to neutral if nan\n",
    "        \n",
    "        # MACD - Moving Average Convergence Divergence: https://www.investopedia.com/terms/m/macd.asp\n",
    "        ## Momentum + trend-following indicator widely used to spot trade direction, strength, as well as buy/sell signals\n",
    "        ema12 = torch.zeros_like(close, device=self.device)\n",
    "        ema26 = torch.zeros_like(close, device=self.device)\n",
    "        alpha12 = 2 / (12 + 1)\n",
    "        alpha26 = 2 / (26 + 1)\n",
    "        ema12[:12] = close[:12].mean()\n",
    "        ema26[:26] = close[:26].mean()\n",
    "        for i in range(12, len(close)):  # short term ema\n",
    "            ema12[i] = alpha12 * close[i] + (1 - alpha12) * ema12[i-1]\n",
    "        for i in range(26, len(close)): # long term ema\n",
    "            ema26[i] = alpha26 * close[i] + (1 - alpha26) * ema26[i-1]\n",
    "        macd = ema12 - ema26\n",
    "        macd = torch.where(torch.isnan(macd), torch.tensor(0.0, device=self.device), macd)\n",
    "        \n",
    "        # ATR - Average True Range: https://www.investopedia.com/terms/a/atr.asp\n",
    "        ## Measure of Volatility\n",
    "        ## Price moves a lot -> High ATR, Price mostly flat -> Low ATR\n",
    "        tr = torch.max(high - low, torch.max(torch.abs(high - close), torch.abs(low - close)))\n",
    "        atr = torch.zeros_like(tr, device=self.device)\n",
    "        for i in range(14, len(tr)): # Wilder's \n",
    "            atr[i] = (atr[i-1] * 13 + tr[i]) / 14\n",
    "        atr = torch.where(torch.isnan(atr), torch.tensor(0.0, device=self.device), atr) # set to neutral if nan\n",
    "        \n",
    "        # Buy Ratio\n",
    "        ## Estimate of how much total trading volume is made up of buying pressure vs. selling\n",
    "        ## Buy Volume: trades that happened at or near ask price -> if relatively high, could signal upward momentum\n",
    "        ## Sell Volume: trades that happed at or near bid price -> if relatively high, could signal downward momentum\n",
    "        buy_ratio = torch.where(volume > 0, buy_volume / volume, torch.tensor(0.5, device=self.device))\n",
    "        \n",
    "        # VWAP Deviation\n",
    "        ## Measure of how far current price is from VWAP\n",
    "        ## Price > VWAP - Buyers pushing up price -> potentially overbought\n",
    "        ## Price < VWAP - Sellers pushing down price -> potentially oversold\n",
    "        ## Large deviations can signal mean reversion\n",
    "        vwap_deviation = (close - vwap) / torch.where(vwap != 0, vwap, torch.tensor(1.0, device=self.device))\n",
    "        vwap_deviation = torch.where(torch.isfinite(vwap_deviation), vwap_deviation, torch.tensor(0.0, device=self.device))\n",
    "        \n",
    "        # Convert torch tensors to dataframes\n",
    "        df['vwap'] = vwap.cpu().numpy()\n",
    "        df['rsi'] = rsi.cpu().numpy()\n",
    "        df['macd'] = macd.cpu().numpy()\n",
    "        df['atr'] = atr.cpu().numpy()\n",
    "        df['buy_ratio'] = buy_ratio.cpu().numpy()\n",
    "        df['vwap_deviation'] = vwap_deviation.cpu().numpy()\n",
    "        return df\n",
    "\n",
    "    def get_single_symbol_kline_data(self, symbol):\n",
    "        try:\n",
    "            df = pd.read_parquet(f\"{self.train_data_path}/{symbol}.parquet\")\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.astype(np.float64)\n",
    "            required_cols = ['open_price', 'high_price', 'low_price', 'close_price', 'volume', 'amount', 'buy_volume']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"{symbol} missing columns: {missing_cols}\")\n",
    "                return pd.DataFrame(columns=required_cols + ['vwap', 'rsi', 'macd', 'buy_ratio', 'vwap_deviation', 'atr'])\n",
    "\n",
    "            # smooth data by removing extreme outliers\n",
    "            df['close_price'] = df['close_price'].clip(df['close_price'].quantile(0.01), df['close_price'].quantile(0.99))\n",
    "            df['volume'] = df['volume'].clip(df['volume'].quantile(0.01), df['volume'].quantile(0.99))\n",
    "\n",
    "            # Calculate the indicators\n",
    "            df = self.compute_factors_torch(df)\n",
    "            print(f\"Loaded data for {symbol}, shape: {df.shape}, vwap NaNs: {df['vwap'].isna().sum()}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {symbol}: {e}\")\n",
    "            return pd.DataFrame(columns=['open_price', 'high_price', 'low_price', 'close_price', 'volume', 'amount', 'buy_volume', 'vwap', 'rsi', 'macd', 'buy_ratio', 'vwap_deviation', 'atr'])\n",
    "\n",
    "    def get_all_symbol_kline(self):\n",
    "        t0 = datetime.datetime.now()\n",
    "        pool = PROCESSES\n",
    "        all_symbol_list = self.get_all_symbol_list()\n",
    "        if not all_symbol_list:\n",
    "            print(\"No symbols found, exiting.\")\n",
    "            pool.close()\n",
    "            return [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        # get symbol data + indicators \n",
    "        df_list = [pool.apply_async(self.get_single_symbol_kline_data, (symbol,)) for symbol in all_symbol_list]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        loaded_symbols = []\n",
    "        for async_result, symbol in zip(df_list, all_symbol_list):\n",
    "            df = async_result.get()\n",
    "            if not df.empty and 'vwap' in df.columns:\n",
    "                loaded_symbols.append(symbol)\n",
    "            else:\n",
    "                print(f\"{symbol} failed: empty or missing 'vwap'\")\n",
    "        failed_symbols = [s for s in all_symbol_list if s not in loaded_symbols]\n",
    "        print(f\"Failed symbols: {failed_symbols}\")\n",
    "\n",
    "        # Clean data\n",
    "        time_index = pd.date_range(start=self.start_datetime, end='2024-12-31', freq='15min') # 15 min time index\n",
    "        df_results = [async_result.get() for async_result in df_list]\n",
    "        df_open_price = pd.concat([\n",
    "            result['open_price'] \n",
    "            for result in df_results \n",
    "            if not result.empty and 'open_price' in result.columns\n",
    "        ], axis=1).sort_index(ascending=True) # get open price dfs for each symbol\n",
    "        print(f\"df_open_price index dtype: {df_open_price.index.dtype}, shape: {df_open_price.shape}\")\n",
    "        df_open_price.columns = loaded_symbols\n",
    "        df_open_price = df_open_price.reindex(columns=all_symbol_list, fill_value=0).reindex(time_index, method='ffill') # unify time steps\n",
    "        time_arr = pd.to_datetime(df_open_price.index).values\n",
    "\n",
    "        def align_df(arr, valid_symbols, key):\n",
    "            valid_dfs = [df[key] for df, s in zip([i.get() for i in df_list], all_symbol_list) if not df.empty and key in df.columns and s in valid_symbols]\n",
    "            if not valid_dfs:\n",
    "                print(f\"No valid data for {key}, filling with zeros\")\n",
    "                return np.zeros((len(time_index), len(all_symbol_list)))\n",
    "            df = pd.concat(valid_dfs, axis=1).sort_index(ascending=True)\n",
    "            df.columns = valid_symbols\n",
    "            return df.reindex(columns=all_symbol_list, fill_value=0).reindex(time_index, method='ffill').values\n",
    "\n",
    "        vwap_arr = align_df(df_list, loaded_symbols, 'vwap')\n",
    "        amount_arr = align_df(df_list, loaded_symbols, 'amount')\n",
    "        atr_arr = align_df(df_list, loaded_symbols, 'atr')\n",
    "        macd_arr = align_df(df_list, loaded_symbols, 'macd')\n",
    "        buy_volume_arr = align_df(df_list, loaded_symbols, 'buy_volume')\n",
    "        volume_arr = align_df(df_list, loaded_symbols, 'volume')\n",
    "\n",
    "        print(f\"Finished get all symbols kline, time elapsed: {datetime.datetime.now() - t0}\")\n",
    "        return all_symbol_list, time_arr, vwap_arr, amount_arr, atr_arr, macd_arr, buy_volume_arr, volume_arr\n",
    "\n",
    "    def weighted_spearmanr(self, y_true, y_pred):\n",
    "        n = len(y_true)\n",
    "        r_true = pd.Series(y_true).rank(ascending=False, method='average')\n",
    "        r_pred = pd.Series(y_pred).rank(ascending=False, method='average')\n",
    "        x = 2 * (r_true - 1) / (n - 1) - 1\n",
    "        w = x ** 2\n",
    "        w_sum = w.sum()\n",
    "        mu_true = (w * r_true).sum() / w_sum\n",
    "        mu_pred = (w * r_pred).sum() / w_sum\n",
    "        cov = (w * (r_true - mu_true) * (r_pred - mu_pred)).sum()\n",
    "        var_true = (w * (r_true - mu_true)**2).sum()\n",
    "        var_pred = (w * (r_pred - mu_pred)**2).sum()\n",
    "        return cov / np.sqrt(var_true * var_pred) if var_true * var_pred > 0 else 0\n",
    "\n",
    "    def train(self, df_target, df_4h_momentum, df_7d_momentum, df_amount_sum, df_vol_momentum, df_atr, df_macd, df_buy_pressure):\n",
    "        factor1_long = df_4h_momentum.stack()\n",
    "        factor2_long = df_7d_momentum.stack()\n",
    "        factor3_long = df_amount_sum.stack()\n",
    "        factor4_long = df_vol_momentum.stack()\n",
    "        factor5_long = df_atr.stack()\n",
    "        factor6_long = df_macd.stack()\n",
    "        factor7_long = df_buy_pressure.stack()\n",
    "        target_long = df_target.stack()\n",
    "\n",
    "        factor1_long.name = '4h_momentum'\n",
    "        factor2_long.name = '7d_momentum'\n",
    "        factor3_long.name = 'amount_sum'\n",
    "        factor4_long.name = 'vol_momentum'\n",
    "        factor5_long.name = 'atr'\n",
    "        factor6_long.name = 'macd'\n",
    "        factor7_long.name = 'buy_pressure'\n",
    "        target_long.name = 'target'\n",
    "\n",
    "        data = pd.concat([factor1_long, factor2_long, factor3_long, factor4_long, factor5_long, factor6_long, factor7_long, target_long], axis=1)\n",
    "        print(f\"Data size before dropna: {len(data)}\")\n",
    "        data = data.replace([np.inf, -np.inf], np.nan).dropna().reset_index(drop=True)\n",
    "        print(f\"Data size after dropna: {len(data)}\")\n",
    "\n",
    "        X = data[['4h_momentum', '7d_momentum', 'amount_sum', 'vol_momentum', 'atr', 'macd', 'buy_pressure']]\n",
    "        y = data['target'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "        X_scaled = self.scaler.fit_transform(X) # stadardize features by z-score \n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        best_score = -np.inf\n",
    "        best_model = None\n",
    "\n",
    "        for train_idx, val_idx in tscv.split(X_scaled):\n",
    "            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            y_train_clean = y_train.fillna(0)\n",
    "            sample_weight = np.where((y_train_clean > y_train_clean.quantile(0.9)) | (y_train_clean < y_train_clean.quantile(0.1)), 2, 1)\n",
    "\n",
    "            model = xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                n_estimators=200,\n",
    "                early_stopping_rounds=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            model.fit(X_train, y_train, sample_weight=sample_weight, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            \n",
    "            y_pred_val = model.predict(X_val)\n",
    "            score = self.weighted_spearmanr(y_val, y_pred_val)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "\n",
    "        print(f\"Best validation Spearman score: {best_score:.4f}\")\n",
    "\n",
    "        data['y_pred'] = best_model.predict(X_scaled)\n",
    "        data['y_pred'] = data['y_pred'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "        data['y_pred'] = data['y_pred'].ewm(span=5).mean()\n",
    "\n",
    "        df_submit = data.reset_index(level=0)\n",
    "        df_submit = df_submit[['level_0', 'y_pred']]\n",
    "        df_submit['symbol'] = df_submit.index.values\n",
    "        df_submit = df_submit[['level_0', 'symbol', 'y_pred']]\n",
    "        df_submit.columns = ['datetime', 'symbol', 'predict_return']\n",
    "        df_submit = df_submit[df_submit['datetime'] >= self.start_datetime]\n",
    "        df_submit[\"id\"] = df_submit[\"datetime\"].dt.strftime(\"%Y%m%d%H%M%S\") + \"_\" + df_submit[\"symbol\"].astype(str)\n",
    "        df_submit = df_submit[['id', 'predict_return']]\n",
    "\n",
    "        if os.path.exists(self.submission_id_path):\n",
    "            df_submission_id = pd.read_csv(self.submission_id_path)\n",
    "            id_list = df_submission_id[\"id\"].tolist()\n",
    "            print(f\"Submission ID count: {len(id_list)}\")\n",
    "            df_submit_competion = df_submit[df_submit['id'].isin(id_list)]\n",
    "            missing_elements = list(set(id_list) - set(df_submit_competion['id']))\n",
    "            print(f\"Missing IDs: {len(missing_elements)}\")\n",
    "            new_rows = pd.DataFrame({'id': missing_elements, 'predict_return': [0] * len(missing_elements)})\n",
    "            df_submit_competion = pd.concat([df_submit_competion, new_rows], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Warning: {self.submission_id_path} not found. Saving submission without ID filtering.\")\n",
    "            df_submit_competion = df_submit\n",
    "\n",
    "        print(\"Submission file sample:\", df_submit_competion.head())\n",
    "        df_submit_competion.to_csv(\"submit.csv\", index=False)\n",
    "\n",
    "        df_check = data.reset_index(level=0)\n",
    "        df_check = df_check[['level_0', 'target']]\n",
    "        df_check['symbol'] = df_check.index.values\n",
    "        df_check = df_check[['level_0', 'symbol', 'target']]\n",
    "        df_check.columns = ['datetime', 'symbol', 'true_return']\n",
    "        df_check = df_check[df_check['datetime'] >= self.start_datetime]\n",
    "        df_check[\"id\"] = df_check[\"datetime\"].dt.strftime(\"%Y%m%d%H%M%S\") + \"_\" + df_check[\"symbol\"]\n",
    "        df_check = df_check[['id', 'true_return']]\n",
    "        df_check.to_csv(\"check.csv\", index=False)\n",
    "\n",
    "        rho_overall = self.weighted_spearmanr(data['target'], data['y_pred'])\n",
    "        print(f\"Weighted Spearman correlation coefficient: {rho_overall:.4f}\")\n",
    "\n",
    "        # SHAP plot\n",
    "        explainer = shap.Explainer(best_model)\n",
    "        shap_values = explainer(X_scaled)\n",
    "        shap.summary_plot(shap_values, X.columns)\n",
    "\n",
    "    def run(self):\n",
    "        all_symbol_list, time_arr, vwap_arr, amount_arr, atr_arr, macd_arr, buy_volume_arr, volume_arr = self.get_all_symbol_kline()\n",
    "        if not all_symbol_list:\n",
    "            print(\"No data loaded, exiting.\")\n",
    "            return\n",
    "\n",
    "        print(f\"all_symbol_list length: {len(all_symbol_list)}, vwap_arr shape: {vwap_arr.shape}\")\n",
    "        df_vwap = pd.DataFrame(vwap_arr, columns=all_symbol_list, index=time_arr)\n",
    "        df_amount = pd.DataFrame(amount_arr, columns=all_symbol_list, index=time_arr)\n",
    "        df_atr = pd.DataFrame(atr_arr, columns=all_symbol_list, index=time_arr)\n",
    "        df_macd = pd.DataFrame(macd_arr, columns=all_symbol_list, index=time_arr)\n",
    "        df_buy_volume = pd.DataFrame(buy_volume_arr, columns=all_symbol_list, index=time_arr)\n",
    "        df_volume = pd.DataFrame(volume_arr, columns=all_symbol_list, index=time_arr)\n",
    "\n",
    "        windows_1d = 4 * 24 * 1\n",
    "        windows_7d = 4 * 24 * 7\n",
    "        windows_4h = 4 * 4\n",
    "\n",
    "        # calculate short-term and long-term momentum\n",
    "        df_4h_momentum = (df_vwap / df_vwap.shift(windows_4h) - 1).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        df_7d_momentum = (df_vwap / df_vwap.shift(windows_7d) - 1).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        # volume factor\n",
    "        df_amount_sum = df_amount.rolling(windows_7d).sum().replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        df_vol_momentum = (df_amount / df_amount.shift(windows_1d) - 1).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        # buy pressure\n",
    "        df_buy_pressure = (df_buy_volume - (df_volume - df_buy_volume)).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        # 24 hour return\n",
    "        df_24hour_rtn = (df_vwap / df_vwap.shift(windows_1d) - 1).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        # TODO: add more factors\n",
    "        \n",
    "        self.train(df_24hour_rtn.shift(-windows_1d), df_4h_momentum, df_7d_momentum, df_amount_sum, df_vol_momentum, df_atr, df_macd, df_buy_pressure)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # mp.set_start_method('spawn', force=True)\n",
    "    \n",
    "    # print(\"Input directory contents:\", os.listdir(\"/kaggle/input/avenir-hku-web/\"))\n",
    "    print(\"Train data directory contents:\", os.listdir(TRAIN_DATA_DIR))\n",
    "    model = OptimizedModel()\n",
    "    model.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
